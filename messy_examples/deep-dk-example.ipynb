{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure stuff is on the path\n",
    "import sys\n",
    "sys.path.append('/Users/paddyslator/python/microtorchfit/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit.py\n",
    "import argparse\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as utils\n",
    "from tqdm import tqdm      \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE (OR FIND!!) A FUNCTION THAT NORMALISES A GENERIC IMAGE!\n",
    "# MAKE SOME SIMULATED DATA TO TRY ON!\n",
    "# NOISE FLOOR \n",
    "# MAGNITUDE/COMPLEX?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = '/Users/paddyslator/Library/CloudStorage/OneDrive-UniversityCollegeLondon/data/brain-dki/'\n",
    "\n",
    "DATADIR = '/Users/paddyslator/Library/CloudStorage/OneDrive-UniversityCollegeLondon/data/HCP/111312_1/T1w/Diffusion/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert Siemens style gradient table to grad\n",
    "siemensgradfilename = 'DiffDir_Spiral_Vec15Dir3bmax2800.txt'\n",
    "\n",
    "def siemens_to_grad(filename,maxb):\n",
    "    grad_dirs = np.loadtxt(filename)\n",
    "    \n",
    "    #scaling factors of the grad_dirs\n",
    "    grad_dirs_scale = np.linalg.norm(grad_dirs,axis=1) ** 2\n",
    "        \n",
    "    #calculate the b-values\n",
    "    bvals = grad_dirs_scale * maxb        \n",
    "    \n",
    "    #normalise the grad_dirs\n",
    "    grad_dirs_scale = grad_dirs_scale.reshape(-1, 1)\n",
    "        \n",
    "    grad_dirs[bvals!=0,:] = grad_dirs[bvals!=0,:] / np.sqrt(grad_dirs_scale[bvals!=0])\n",
    "    \n",
    "    grad = np.concatenate((grad_dirs,bvals[:,None]),axis=1)\n",
    "    \n",
    "    return grad\n",
    "    \n",
    "    \n",
    "grad = siemens_to_grad(DATADIR + siemensgradfilename, 2800)\n",
    "\n",
    "np.savetxt(DATADIR + siemensgradfilename.split('.')[0] + '_grad.txt', grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgfilename = 'imageMatrixCpx.nii.gz'\n",
    "maskfilename = 'imageMatrixCpx_mask.nii.gz'\n",
    "gradfilename = 'DiffDir_Spiral_Vec15Dir3bmax2800_grad.txt'\n",
    "\n",
    "imgfilename = 'data.nii.gz'\n",
    "maskfilename = 'nodif_brain_mask.nii.gz'\n",
    "gradfilename = 'grad.b'\n",
    "\n",
    "\n",
    "imgnii = nib.load(DATADIR + imgfilename)\n",
    "img = imgnii.get_fdata()\n",
    "\n",
    "masknii = nib.load(DATADIR + maskfilename)\n",
    "mask = masknii.get_fdata()\n",
    "\n",
    "grad = np.loadtxt(DATADIR + gradfilename)\n",
    "\n",
    "#round b-values to nearest integer\n",
    "grad[:,3] = np.round(grad[:,3])\n",
    "\n",
    "#convert to microns\n",
    "grad[:,3]= 1e-3 * grad[:,3]\n",
    "\n",
    "\n",
    "#grad[:,3] = 1e-3*(grad[:,3] - np.min(grad[:,3]))\n",
    "\n",
    "#remove the first n images\n",
    "\n",
    "# #0 removes 1, 1 removes 2 etc\n",
    "# n_start = 0\n",
    "\n",
    "# img = img[:,:,:,n_start:]\n",
    "# grad = grad[n_start:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dipy\n",
    "#baseline with dipy implementation\n",
    "\n",
    "#make gradient table\n",
    "from dipy.core.gradients import gradient_table\n",
    "gtab = gradient_table(1e3 * grad[:,3], grad[:,0:3])\n",
    "\n",
    "# Reconstruction modules\n",
    "import dipy.reconst.msdki as msdki\n",
    "\n",
    "msdki_model = msdki.MeanDiffusionKurtosisModel(gtab)\n",
    "\n",
    "\n",
    "#do the fit\n",
    "msdki_fit = msdki_model.fit(img, mask)\n",
    "\n",
    "#save the maps as nifti\n",
    "MSD = msdki_fit.msd\n",
    "MSK = msdki_fit.msk\n",
    "\n",
    "#save the inferred maps as niftis\n",
    "maps = np.stack((MSD, MSK),axis=-1)\n",
    "\n",
    "#use the image as a template\n",
    "mapsnii = nib.Nifti1Image(maps, affine=imgnii.affine,header=imgnii.header)\n",
    "#adjust 4th spatial dimension\n",
    "mapsnii.header['dim'][4] = np.shape(maps)[-1]\n",
    "\n",
    "nib.save(mapsnii, DATADIR + imgfilename[0:-7] + '_DIPY_DK_maps.nii.gz')  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10,5))\n",
    "\n",
    "zslice=70\n",
    "\n",
    "plt0 = ax[0].imshow(MSK[:,:,zslice],vmin=0,vmax=2)\n",
    "# plt.colorbar(plt0,ax=ax[0])\n",
    "# ax[0].xaxis.set_ticklabels([]) \n",
    "# ax[0].set_title('diffusivity ($\\mu$m$^2$/ms)')\n",
    "# #ax[0].set_title('diffusivity (mm$^2$/ms)')\n",
    "# ax[0].axis('off')\n",
    "\n",
    "# plt0 = ax[1].imshow(T2_map[:,:,zslice],cmap='hot',vmin=0,vmax=.06)\n",
    "# plt.colorbar(plt0,ax=ax[1])    \n",
    "# ax[1].xaxis.set_ticklabels([]) \n",
    "# ax[1].set_title('T2 (s)')\n",
    "# ax[1].axis('off')\n",
    "    \n",
    "plt0 = ax[1].imshow(MSD[:,:,zslice],cmap='plasma')\n",
    "# plt.colorbar(plt0,ax=ax[1])    \n",
    "# ax[1].xaxis.set_ticklabels([]) \n",
    "# ax[1].set_title('kurtosis')\n",
    "# ax[1].axis('off') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess data\n",
    "from utils.preprocessing import direction_average\n",
    "\n",
    "#take the spherical mean\n",
    "da_img,da_grad = direction_average(img,grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a smaller mask for testing\n",
    "tmpmask = np.zeros_like(mask)\n",
    "tmpmask[:,:,70] = mask[:,:,70]\n",
    "mask=tmpmask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess for machine learning! MAKE A FUNCTION OF THIS!\n",
    "#(todo)\n",
    "\n",
    "#define some useful functions\n",
    "def normalise(X_train,grad):\n",
    "    nvol = np.shape(grad)[0]\n",
    "    \n",
    "    #normalise \n",
    "    #find the volumes to normalise by - the lowest b-value lowest TE volume\n",
    "    #ADD SOME TOLERANCE TO THIS\n",
    "    #normvol = np.where((grad[:,3] == min(grad[:,3])) & (grad[:,4]==min(grad[:,4])))\n",
    "    \n",
    "    #this just works for diffusion MRI - need to change if multiple echo times etc.\n",
    "    normvol = np.where(grad[:,3] == min(grad[:,3]))[0]\n",
    "                       \n",
    "    if len(normvol)>1:\n",
    "        X_train = X_train/(np.tile(np.mean(X_train[:,normvol], axis=1),(1, nvol)))\n",
    "    else:\n",
    "        X_train = X_train/(np.tile(X_train[:,normvol],(1, nvol)))\n",
    "    \n",
    "    return X_train\n",
    "\n",
    "\n",
    "def img2voxel(img,mask):\n",
    "    nvoxtotal = np.prod(np.shape(img)[0:3])\n",
    "    nvol = np.shape(img)[3]\n",
    "    #image in voxel format\n",
    "    imgvox = np.reshape(img,(nvoxtotal,nvol))\n",
    "    #mask in voxel format\n",
    "    maskvox = np.reshape(mask,(nvoxtotal))\n",
    "    #extract the voxels in the mask\n",
    "    X_train = imgvox[maskvox==1]    \n",
    "    \n",
    "    return X_train,maskvox\n",
    "\n",
    "\n",
    "#flatten/voxelise\n",
    "X_train,maskvox = img2voxel(da_img,mask)\n",
    "\n",
    "print(np.shape(X_train))\n",
    "print(np.shape(maskvox))\n",
    "\n",
    "\n",
    "# nvoxtotal = np.prod(np.shape(da_img)[0:3])\n",
    "# nvol = np.shape(da_img)[3]\n",
    "# #image in voxel format\n",
    "# imgvox = np.reshape(da_img,(nvoxtotal,nvol))\n",
    "# #mask in voxel format\n",
    "# maskvox = np.reshape(mask,(nvoxtotal))\n",
    "# #extract the voxels in the mask\n",
    "# X_train = imgvox[maskvox==1]\n",
    "\n",
    "#normalise using the function\n",
    "X_train = normalise(X_train,da_grad)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #simulated data to try the model on \n",
    "# import numpy as np\n",
    "\n",
    "# #simulate some data from a \"cluster model\"\n",
    "# nvox = 1024\n",
    "# nclus = 5\n",
    "# p = [0.1, 0.1, 0.2, 0.5]\n",
    "\n",
    "\n",
    "# p = np.append(p,1-np.sum(p))\n",
    "# clusters = np.random.choice(range(0,nclus),size=(nvox,),p=p)\n",
    "\n",
    "# #define the underlying tissue parameters for each cluster\n",
    "# D = [0.5,1,1.5,2,3]\n",
    "# K = [1,0.5,0.2,0.1,0.01]\n",
    "# #K = [0.1,0.05,0.2,0.1,0]\n",
    "\n",
    "# mu = np.stack((D,K))\n",
    "# var = np.diag([0.01,0.01])\n",
    "\n",
    "\n",
    "# params = np.zeros((nvox,2))\n",
    "\n",
    "# for vox in range(0,nvox):\n",
    "#     params[vox,:] = np.random.multivariate_normal(mu[:,clusters[vox]],var)\n",
    "    \n",
    "\n",
    "# from signal_models import msdki\n",
    "\n",
    "# tor_params = torch.from_numpy(params)\n",
    "# tor_grad = torch.from_numpy(da_grad) \n",
    "# tor_grad = tor_grad.to(torch.float32)\n",
    "\n",
    "# S = msdki(tor_grad,tor_params)\n",
    "\n",
    "# X_train = S.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nparams = 2\n",
    "#define the neural network - change to import this from elsewhere! \n",
    "class Net_test(nn.Module):\n",
    "    def __init__(self, grad, nparams): #PASS MODEL STRING AS AN ARGUMENT IN HERE!\n",
    "        super(Net_test, self).__init__()\n",
    "\n",
    "        self.grad = grad\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        \n",
    "        for i in range(3): # 3 fully connected hidden layers\n",
    "            self.fc_layers.extend([nn.Linear(grad.size(0), grad.size(0)), nn.ELU()])\n",
    "        self.encoder = nn.Sequential(*self.fc_layers, nn.Linear(grad.size(0), nparams))\n",
    "        \n",
    "        #self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, X):\n",
    "        #X = self.dropout(X)\n",
    "        params = torch.abs(self.encoder(X)) # D, T2, K\n",
    "        D = torch.clamp(params[:, 0].unsqueeze(1), min=0.001, max=3)\n",
    "        K = torch.clamp(params[:, 1].unsqueeze(1), min=0.001, max=2)\n",
    "         \n",
    "#         D = params[:, 0].unsqueeze(1)\n",
    "#         K = params[:, 1].unsqueeze(1)                \n",
    "        \n",
    "        bvals = self.grad[:,3]\n",
    "        \n",
    "        X = torch.exp(-bvals*D + 1/6 * bvals**2 * D**2 * K )\n",
    "        \n",
    "\n",
    "        \n",
    "        return X, D, K\n",
    "    \n",
    "    \n",
    "\n",
    "# make the Network\n",
    "grad = torch.FloatTensor(da_grad)\n",
    "net_test = Net_test(grad, nparams)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the neural network using the functions    \n",
    "\n",
    "#define the model\n",
    "#comps = (\"MSDKI\",)\n",
    "comps = (\"Ball\",\"Stick\")\n",
    "\n",
    "#import dynamically\n",
    "import importlib\n",
    "signal_models_module = importlib.import_module(\"signal_models\")\n",
    "\n",
    "comps_classes = () #initialise tuple\n",
    "for comp in comps:\n",
    "    #get the class\n",
    "    this_class = getattr(signal_models_module, comp) #add to the tuple\n",
    "    #create an instance of the class and add to the tuple\n",
    "    comps_classes += (this_class(),)\n",
    "\n",
    "from model_maker import ModelMaker\n",
    "\n",
    "modelfunc = ModelMaker(comps_classes)\n",
    "\n",
    "import torch.nn as nn\n",
    "from utils.net_maker import Net\n",
    "\n",
    "#make an example of the network\n",
    "net = Net(grad, modelfunc, dim_hidden=grad.shape[0], num_layers=3, dropout_frac=0, activation=nn.ELU())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfunc.n_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thing = np.mean(X_train,axis=1)\n",
    "np.shape(thing)\n",
    "np.shape(np.tile(thing,(18,1)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teston = False\n",
    "\n",
    "\n",
    "#initialise the weights\n",
    "# def weights_init(m):\n",
    "#     if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "#         nn.init.xavier_uniform_(m.weight.data)\n",
    "#         nn.init.zeros_(m.bias.data)\n",
    "\n",
    "# net.apply(weights_init)\n",
    "\n",
    "\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "if teston:\n",
    "    optimizer = optim.Adam(net_test.parameters(), lr = 0.01)  \n",
    "else:\n",
    "    optimizer = optim.Adam(net.parameters(), lr = 0.01)  \n",
    "\n",
    "    \n",
    "#optimizer = optim.SGD(net.parameters(), lr = 0.01)  \n",
    "\n",
    "\n",
    "#create batch queues\n",
    "batch_size = 128\n",
    "num_batches = len(X_train) // batch_size\n",
    "\n",
    "#X_train = X_train[:,1:] # exlude the b=0 value as signals are normalized\n",
    "\n",
    "trainloader = utils.DataLoader(torch.from_numpy(X_train.astype(np.float32)),\n",
    "                            batch_size = batch_size, \n",
    "                            shuffle = True,\n",
    "                            num_workers = 2,\n",
    "                            drop_last = True)\n",
    "\n",
    "\n",
    "#learning rate scheduler\n",
    "# from torch.optim import lr_scheduler\n",
    "# scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Best loss\n",
    "best = 1e16\n",
    "num_bad_epochs = 0\n",
    "patience = 10\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "              \n",
    "# Train\n",
    "for epoch in range(100): \n",
    "    print(\"-----------------------------------------------------------------\")\n",
    "    print(\"Epoch: {}; Bad epochs: {}\".format(epoch, num_bad_epochs))\n",
    "    if teston:\n",
    "        net_test.train()\n",
    "    else:\n",
    "        net.train()\n",
    "        \n",
    "    running_loss = 0.\n",
    "\n",
    "    for i, X_batch in enumerate(tqdm(trainloader), 0):\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        #X_pred, Dp_pred, Dt_pred, Fp_pred = net(X_batch)\n",
    "        \n",
    "        if teston:\n",
    "            X_pred, D_pred, K_pred = net_test(X_batch)\n",
    "        else:\n",
    "            X_pred, params_pred = net(X_batch)\n",
    "        \n",
    "        \n",
    "        loss = criterion(X_pred, X_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(\"Loss: {}\".format(running_loss))\n",
    "    # early stopping\n",
    "    if running_loss < best:\n",
    "        print(\"############### Saving good model ###############################\")\n",
    "        if teston:\n",
    "            final_model = net_test.state_dict()\n",
    "        else:\n",
    "            final_model = net.state_dict()\n",
    "            \n",
    "        best = running_loss\n",
    "        num_bad_epochs = 0\n",
    "    else:\n",
    "        num_bad_epochs = num_bad_epochs + 1\n",
    "        if num_bad_epochs == patience:\n",
    "            print(\"Done, best loss: {}\".format(best))\n",
    "            break\n",
    "print(\"Done\")\n",
    "# Restore best model\n",
    "if teston:\n",
    "    net_test.load_state_dict(final_model)\n",
    "else:\n",
    "    net.load_state_dict(final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inference\n",
    "if teston:\n",
    "    net_test.eval()\n",
    "else:\n",
    "    net.eval()\n",
    "    \n",
    "with torch.no_grad():\n",
    "    if teston:\n",
    "        X, D, K = net_test(torch.from_numpy(X_train.astype(np.float32)))\n",
    "        \n",
    "        D = D.numpy()\n",
    "        K = K.numpy()\n",
    "      \n",
    "        plt.plot(tor_params[:,0],D,'o')\n",
    "      #plt.plot(K)\n",
    "      \n",
    "      #convert parameters back to image format\n",
    "      # D_vox = np.zeros(np.shape(maskvox))\n",
    "      # D_vox[maskvox==1] = np.squeeze(D[:])\n",
    "      # D_map = np.reshape(D_vox,np.shape(mask))\n",
    "\n",
    "      # K_vox = np.zeros(np.shape(maskvox))\n",
    "      # K_vox[maskvox==1] = np.squeeze(K[:])\n",
    "      # K_map = np.reshape(K_vox,np.shape(mask))\n",
    "    else:\n",
    "        X, params = net(torch.from_numpy(X_train.astype(np.float32)))\n",
    "        params = params.numpy()\n",
    "      \n",
    "        #plt.plot(tor_params[:,0],params[:,0],'o')\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #rotate images\n",
    "# from scipy import ndimage\n",
    "\n",
    "# D_map = ndimage.rotate(D_map,-90,reshape=True)\n",
    "# K_map = ndimage.rotate(K_map,-90,reshape=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_map = np.zeros((*np.shape(mask),2))\n",
    "\n",
    "print(np.shape(param_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpparams = np.zeros_like(maskvox)\n",
    "tmpparams[maskvox==1] = 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10,5))\n",
    "\n",
    "if teston:\n",
    "    zslice=28\n",
    "else:\n",
    "    D_map = \n",
    "    \n",
    "    \n",
    "    plt0 = ax[0].imshow(D_map[:,:,zslice],vmin=0,vmax=3)\n",
    "    plt.colorbar(plt0,ax=ax[0])\n",
    "    ax[0].xaxis.set_ticklabels([]) \n",
    "    ax[0].set_title('diffusivity ($\\mu$m$^2$/ms)')\n",
    "    #ax[0].set_title('diffusivity (mm$^2$/ms)')\n",
    "    ax[0].axis('off')\n",
    "\n",
    "    # plt0 = ax[1].imshow(T2_map[:,:,zslice],cmap='hot',vmin=0,vmax=.06)\n",
    "    # plt.colorbar(plt0,ax=ax[1])    \n",
    "    # ax[1].xaxis.set_ticklabels([]) \n",
    "    # ax[1].set_title('T2 (s)')\n",
    "    # ax[1].axis('off')\n",
    "\n",
    "    plt0 = ax[1].imshow(K_map[:,:,zslice],cmap='plasma',vmin=0,vmax=2)\n",
    "    plt.colorbar(plt0,ax=ax[1])    \n",
    "    ax[1].xaxis.set_ticklabels([]) \n",
    "    ax[1].set_title('kurtosis')\n",
    "    ax[1].axis('off')    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the inferred maps as niftis\n",
    "maps = np.stack((D_map, K_map),axis=-1)\n",
    "\n",
    "#use the image as a template\n",
    "mapsnii = nib.Nifti1Image(maps, affine=imgnii.affine,header=imgnii.header)\n",
    "#adjust 4th spatial dimension\n",
    "mapsnii.header['dim'][4] = np.shape(maps)[-1]\n",
    "\n",
    "nib.save(mapsnii, DATADIR + imgfilename[0:-7] + '_DK_maps.nii.gz')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[1000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion(X_pred, X_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(grad[:,3],X_batch[1,:].detach().numpy(),'o')\n",
    "plt.plot(grad[:,3],X_pred[1,:].detach().numpy(),'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "1ca6ec867e98636ca0c4c75c3da984a3329c9a5ee8b915c41f07d90eaab86bba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
